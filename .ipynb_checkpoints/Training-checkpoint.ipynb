{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pylab as plt\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from Data_Loading import *\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('Banco de Dados/atributos/atributos_main.csv', index_col=0)\n",
    "main_df['filepath'] = main_df['database']+'/'+main_df['filename']\n",
    "main_df = main_df.set_index(['database', 'filename'])\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['bpm'] = main_df['bpm'].apply(lambda bpm: round(bpm*2)/2)\n",
    "\n",
    "X = main_df.loc[:, :'v_sd_D1']\n",
    "y = main_df.loc[:, ['bpm']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "y_train = y_train.values.reshape(-1,)\n",
    "y_test = y_test.values.reshape(-1,)\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(y_train, bins = 273)\n",
    "plt.subplot(2,1,2)\n",
    "plt.hist(y_test, bins = 273)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dict = {}\n",
    "\n",
    "for bpm in main_df['bpm'].unique():\n",
    "    normal_dict[bpm] = main_df[main_df['bpm'] == bpm]\n",
    "\n",
    "\n",
    "\n",
    "random_df = main_df.sample(frac=1)\n",
    "rnd_dict = {}\n",
    "\n",
    "for bpm in random_df['bpm'].unique():\n",
    "    rnd_dict[bpm] = random_df[random_df['bpm'] == bpm]\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicio = rnd_dict #Choose dictionary between random or not\n",
    "\n",
    "train = pd.DataFrame()\n",
    "cv = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "first = True\n",
    "\n",
    "for lab, bpm_list in dicio.items():\n",
    "    if(len(bpm_list)<10):\n",
    "        train = pd.concat([train, bpm_list], axis=0)\n",
    "    else:\n",
    "        train_size = int(0.7*len(bpm_list))\n",
    "        cv_size = int(0.15*len(bpm_list))\n",
    "        test_size = int(0.15*len(bpm_list))\n",
    "        \n",
    "        train_set = bpm_list.iloc[:train_size]\n",
    "        cv_set = bpm_list.iloc[train_size-1:train_size+cv_size]\n",
    "        test_set = bpm_list.iloc[train_size+cv_size-1:]\n",
    "        \n",
    "        train = pd.concat([train, train_set], axis=0)\n",
    "        cv = pd.concat([cv, cv_set], axis=0)\n",
    "        test = pd.concat([test, test_set], axis=0)\n",
    "        \n",
    "        \n",
    "X_train = train.loc[:, :'v_sd_D1']\n",
    "y_train = train.loc[:, ['bpm']]\n",
    "\n",
    "X_cv = cv.loc[:, :'v_sd_D1']\n",
    "y_cv = cv.loc[:, ['bpm']]\n",
    "\n",
    "X_test = test.loc[:, :'v_sd_D1']\n",
    "y_test = test.loc[:, ['bpm']]\n",
    "\n",
    "\n",
    "y_train = y_train.values.reshape(-1,)\n",
    "y_cv = y_cv.values.reshape(-1,)\n",
    "y_test = y_test.values.reshape(-1,)\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.hist(y_train, bins = 273)\n",
    "plt.subplot(3,1,2)\n",
    "plt.hist(y_cv, bins = 273)\n",
    "plt.subplot(3,1,3)\n",
    "plt.hist(y_test, bins = 273)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = StandardScaler().fit_transform(X_train)\n",
    "X_cv_std = StandardScaler().fit_transform(X_cv)\n",
    "X_test_std = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame({\"Espaço\":[],\n",
    "                           \"Train_loss_array\":[], \"Train_loss\":[],\n",
    "                           \"CV_loss_array\":[], \"CV_loss\":[],\n",
    "                           \"Test_loss_array\":[], \"Test_loss\":[],\n",
    "                           \"num_iter\":[], \"time_elapsed\":[]})\n",
    "\n",
    "\n",
    "\n",
    "#alphas = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]\n",
    "#learning_rates = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3]\n",
    "#layer_sizes = [(3,), (3,3), (5,), (5,5), (7,), (7,7), (9,), (9,9)]\n",
    "\n",
    "alphas = [1, 3, 10, 30, 100, 300]\n",
    "learning_rates = [0.0003, 0.001, 0.003, 0.01, 0.03]\n",
    "layer_sizes = [(5,), (5,5), (7,), (7,7), (9,), (9,9)]\n",
    "\n",
    "iter_divisor = 24\n",
    "\n",
    "params_space = list(product(alphas, learning_rates, layer_sizes))\n",
    "params_num = len(params_space)\n",
    "cur_param = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (113909657.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_9213/113909657.py\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    else\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for space_i, space in enumerate(params_space):\n",
    "    start = time.time()\n",
    "    mlp = MLPRegressor(alpha = space[0], learning_rate_init = space[1], hidden_layer_sizes = space[2], warm_start = True)\n",
    "    train_error = []\n",
    "    cv_error = []\n",
    "    test_error = []\n",
    "    \n",
    "    times_down = 0\n",
    "    times_up = 0\n",
    "    max_iter = int(iter_divisor/space[1])\n",
    "    for i in range(max_iter):\n",
    "        mlp.partial_fit(X_train_std, y_train)\n",
    "        train_loss = mean_squared_error(mlp.predict(X_train_std), y_train)\n",
    "        \n",
    "        if i>0:\n",
    "            if(train_error[-1] - train_loss)<0:\n",
    "                times_up =  times_up+1\n",
    "                if times_up >=5:\n",
    "                    sys.stdout.flush()\n",
    "                    print(f'Espaço de Parâmetros {space_i}: {space} - Treinamento encerrado por falta de evolução: {i} iterações | erro={round(train_loss,3)}!')\n",
    "                    break\n",
    "            elif (train_error[-1] - train_loss)<0.001:\n",
    "                times_down  = times_down + 1\n",
    "                if times_down >=5:\n",
    "                    sys.stdout.flush()\n",
    "                    print(f'Espaço de Parâmetros {space_i}: {space} - Treinamento encerrado por evolução aceitável: {i} iterações | erro={round(train_loss,3)}!')\n",
    "                    break                \n",
    "            else:\n",
    "                times_down=0\n",
    "                times_up=0\n",
    "                    \n",
    "        \n",
    "        train_error.append(train_loss)\n",
    "        cv_error.append(mean_squared_error(mlp.predict(X_cv_std), y_cv))\n",
    "        test_error.append(mean_squared_error(mlp.predict(X_test_std), y_test))\n",
    "        \n",
    "        print(f'Espaço de Parâmetros {space_i} {space} - Iterações realizadas: {i}/{max_iter} - Train_loss: {train_loss}', end='\\r')\n",
    "        \n",
    "        if i==max_iter-1:\n",
    "            sys.stdout.flush()\n",
    "            print(f'Espaço de Parâmetros {space_i}: {space} - Treinamento encerrado por máximo de iterações: {max_iter}!')\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    \n",
    "    this_dict = {\"Espaço\":[], \"Layers\":[], \"Train_loss_array\":[], \"CV_loss_array\":[], \"Test_loss_array\":[]}\n",
    "    this_dict[\"Espaço\"].append(space)\n",
    "    this_dict[\"Alpha\"] = space[0]\n",
    "    this_dict[\"Learning Rate\"] = space[1]\n",
    "    this_dict[\"Layers\"].append(space[2])\n",
    "    this_dict[\"Train_loss_array\"].append(train_error)\n",
    "    this_dict[\"Train_loss\"] = float(train_error[-1])\n",
    "    this_dict[\"CV_loss_array\"].append(cv_error)\n",
    "    this_dict[\"CV_loss\"] = float(cv_error[-1])\n",
    "    this_dict[\"Test_loss_array\"].append(test_error)\n",
    "    this_dict[\"Test_loss\"] = float(test_error[-1])\n",
    "    this_dict[\"num_iter\"] = i\n",
    "    this_dict[\"time_elapsed\"] = (end-start)/60\n",
    "    \n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame(this_dict)], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(['CV_loss','num_iter'], ascending = [True, False]).to_csv('result_of_48_iterations_v3.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three = pd.read_csv('result_of_48_iterations_v3.csv')\n",
    "three.sort_values('CV_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = three.groupby('Alpha')['CV_loss'].agg([np.mean, np.min, np.max])\n",
    "alpha['score'] = (alpha['mean']+alpha['amin'])/2\n",
    "print('Alpha')\n",
    "print(alpha.sort_values('score').iloc[0])\n",
    "print(alpha.sort_values('mean').iloc[0])\n",
    "print(alpha.sort_values('amin').iloc[0])\n",
    "print()\n",
    "print('Layers')\n",
    "layers = three.groupby('Layers')['CV_loss'].agg([np.mean, np.min, np.max])\n",
    "layers['score'] = (layers['mean']+layers['amin'])/2\n",
    "print(layers.sort_values('score').iloc[0])\n",
    "print(layers.sort_values('mean').iloc[0])\n",
    "print(layers.sort_values('amin').iloc[0])\n",
    "print()\n",
    "print('Learning Rate')\n",
    "learning_rate = three.groupby('Learning Rate')['CV_loss'].agg([np.mean, np.min, np.max])\n",
    "learning_rate['score'] = (learning_rate['mean']+learning_rate['amin'])/2\n",
    "print(learning_rate.sort_values('score').iloc[0])\n",
    "print(learning_rate.sort_values('mean').iloc[0])\n",
    "print(learning_rate.sort_values('amin').iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = main_df.loc[main_df['database']!= 'extended_ballroom']\n",
    "test_df['database'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = main_df.loc[main_df['database']!= 'extended_ballroom']\n",
    "\n",
    "X = test_df.iloc[:, :66]\n",
    "y = test_df.iloc[:, [66]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "y_train = y_train.values.reshape(-1,)\n",
    "y_test = y_test.values.reshape(-1,)\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(y_train, bins = 273)\n",
    "plt.subplot(2,1,2)\n",
    "plt.hist(y_test, bins = 273)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha = 0.4, normalize=True)\n",
    "lasso.fit(X,y)\n",
    "print(lasso.coef_)\n",
    "\n",
    "plt.plot(lasso.coef_, range(len(X.columns)))\n",
    "plt.yticks(range(len(X.columns)), X.columns, rotation = -15)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import seaborn as sns\n",
    "#sns.heatmap(main_df.corr(), square=True, cmap='RdYlGn')\n",
    "\n",
    "main_df.corr().loc[:, ['bpm']].iloc[:-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(max_iter=2000)\n",
    "\n",
    "norm = StandardScaler()\n",
    "quantile = QuantileTransformer()\n",
    "\n",
    "mse = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "mae = make_scorer(mean_absolute_error,  greater_is_better=False)\n",
    "r2 = make_scorer(r2_score)\n",
    "\n",
    "param_grid_mlp = {'model__hidden_layer_sizes': [(5,), (5,5)],\n",
    "                  'model__learning_rate_init': [0.01, 0.03, 0.1],\n",
    "                  'model__alpha': [0.003, 0.01, 0.03, 0.1]}\n",
    "\n",
    "\n",
    "#MLP com Standard Scaler\n",
    "mlp_scale_pipe= Pipeline([\n",
    "    ('scale', norm),\n",
    "    ('model', mlp)\n",
    "])\n",
    "\n",
    "mlp_scale = GridSearchCV(estimator = mlp_scale_pipe,\n",
    "                   param_grid = param_grid_mlp,\n",
    "                   scoring = {'mse': mse, 'mae': mae, 'r2': r2},\n",
    "                   refit='r2',\n",
    "                   cv=10,\n",
    "                   n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "#MLP com Quantile Transform\n",
    "mlp_quant_pipe = Pipeline([\n",
    "    ('scale', quantile),\n",
    "    ('model', mlp)\n",
    "])\n",
    "\n",
    "mlp_quant = GridSearchCV(estimator = mlp_quant_pipe,\n",
    "                   param_grid = param_grid_mlp,\n",
    "                   scoring = {'mse': mse, 'mae': mae, 'r2': r2},\n",
    "                   refit='r2',\n",
    "                   cv=10,\n",
    "                   n_jobs=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "mlp_scale.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "\n",
    "print(f'MLP com Standard Scaler Treinado em {(end-start)/60} minutos')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "mlp_quant.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "\n",
    "print(f'MLP com Quantile Transformer Treinado em {(end-start)/60} minutos')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(mlp_scale.best_score_)\n",
    "print(mlp_scale.best_params_)\n",
    "print()\n",
    "\n",
    "print(mlp_quant.best_score_)\n",
    "print(mlp_quant.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mlp_scale.cv_results_).loc[:,['mean_test_mse', 'rank_test_mse', 'mean_test_mae', 'rank_test_mae', 'mean_test_r2', 'rank_test_r2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = StandardScaler().fit_transform(X_train)\n",
    "X_test_std = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly = PolynomialFeatures(2).fit_transform(X_train)\n",
    "X_test_poly = PolynomialFeatures(2).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "\n",
    "mlp_alone = MLPRegressor(max_iter = 1000, alpha = 0.3, hidden_layer_sizes = (5,), learning_rate_init = 0.001, warm_start = True)\n",
    "\n",
    "times = 0\n",
    "\n",
    "for i in range(5500):\n",
    "    mlp_alone.partial_fit(X_train_std, y_train)\n",
    "    train_loss = mean_squared_error(mlp_alone.predict(X_train_std), y_train)\n",
    "    \n",
    "    evolution = 0\n",
    "    if i>0:    \n",
    "        evolution = train[-1]-train_loss\n",
    "        if np.abs(evolution) < 0.001:\n",
    "            times = times + 1\n",
    "            if times > 10:\n",
    "                sys.stdout.flush()\n",
    "                print(f'Treinamento encerrado por evolução aceitável: {evolution} com {i} iterações!')\n",
    "                break\n",
    "            else:\n",
    "                times = 0\n",
    "            \n",
    "            \n",
    "    train.append(train_loss)\n",
    "    pred = mlp_alone.predict(X_test_std)\n",
    "    test_loss  = mean_squared_error(pred, y_test)\n",
    "    test.append(test_loss) \n",
    "    print(f'Iterações realizadas: {i} ---- Evolução: {evolution}', end='\\r')\n",
    "    \n",
    "if i==1199:\n",
    "    print('Treinamento encerrado por máximo de iterações!')\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(i+1), test, label='teste')\n",
    "plt.plot(range(i+1), train, label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(mlp_alone.predict(X_test_std), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bancos = {}\n",
    "bancos_names = main_df['database'].unique()\n",
    "\n",
    "for banco in bancos_names:\n",
    "    banco_i = main_df.loc[main_df['database']==banco]\n",
    "    bancos[banco] = banco_i.iloc[:, :67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bancos_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bancos['banco1']['bpm'], bins = 150);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for banco in bancos:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
